ğŸŒ¤ï¸ Pipeline ETL - Dados ClimÃ¡ticos de SÃ£o Paulo

> Pipeline ETL automatizado para coleta, transformaÃ§Ã£o e armazenamento de dados meteorolÃ³gicos em tempo real da cidade de SÃ£o Paulo.

---

## ğŸ“‹ Ãndice

- [Sobre o Projeto](#-sobre-o-projeto)
- [Arquitetura do Pipeline](#-arquitetura-do-pipeline)
- [Stack TecnolÃ³gica](#-stack-tecnolÃ³gica)
- [Estrutura do Projeto](#-estrutura-do-projeto)
- [PrÃ©-requisitos](#-prÃ©-requisitos)
- [InstalaÃ§Ã£o e ConfiguraÃ§Ã£o](#-instalaÃ§Ã£o-e-configuraÃ§Ã£o)
- [Como Executar](#-como-executar)
- [Detalhamento das Etapas](#-detalhamento-das-etapas)
- [AnÃ¡lise de Dados](#-anÃ¡lise-de-dados)

---

## ğŸ¯ Sobre o Projeto

Este projeto foi desenvolvido com o objetivo de demonstrar a construÃ§Ã£o de um **pipeline ETL completo** utilizando as melhores prÃ¡ticas de Engenharia de Dados.

O pipeline coleta dados meteorolÃ³gicos da API OpenWeatherMap a cada hora, transforma os dados para um formato estruturado e os armazena em um banco de dados MySQL para anÃ¡lises futuras.



## ğŸ—ï¸ Arquitetura do Pipeline

<img src='arquitetura_de_dados_draw.png' alt='Arquitetura do Pipeline ETL'>

---

## ğŸ› ï¸ Stack TecnolÃ³gica

### Core
- **Python 3.14+** - Linguagem principal
- **Apache Airflow 3.1.7** - OrquestraÃ§Ã£o do pipeline
- **MySQL** - Banco de dados relacional
- **Docker & Docker Compose** - ContainerizaÃ§Ã£o

### Bibliotecas Python
- **pandas** - ManipulaÃ§Ã£o e transformaÃ§Ã£o de dados
- **requests** - RequisiÃ§Ãµes HTTP para a API
- **SQLAlchemy** - ORM para interaÃ§Ã£o com o banco de dados
- **mysql-connector-python** - Driver MySQL
- **python-dotenv** - Gerenciamento de variÃ¡veis de ambiente

### Outras Ferramentas
- **Redis** - Message broker para Celery
- **Jupyter Notebook** - AnÃ¡lise exploratÃ³ria de dados
- **UV** - Gerenciador de pacotes Python rÃ¡pido

---

## ğŸš€ InstalaÃ§Ã£o e ConfiguraÃ§Ã£o

### 1ï¸âƒ£ Clone o RepositÃ³rio

```bash
git clone https://github.com/Enzoonofre/weather_pipeline_ETL.git
cd weather_pipeline_ETL
```

### 2ï¸âƒ£ Obtenha sua API Key do OpenWeatherMap

1. Acesse [OpenWeatherMap](https://openweathermap.org/api)
2. Crie uma conta gratuita
3. Gere sua API Key no dashboard
4. Guarde sua chave para o prÃ³ximo passo

### 3ï¸âƒ£ Configure as VariÃ¡veis de Ambiente

Crie um arquivo `.env` dentro da pasta `config/`:

```bash
# config/.env

# OpenWeatherMap API
API_KEY=sua_chave_api_aqui

# MySQL (para testes locais)
user=airflow
password=airflow
database=airflow
```

> âš ï¸ **IMPORTANTE**: Nunca commite o arquivo `.env` no Git!

### 4ï¸âƒ£ Inicialize o Ambiente Airflow

```bash
# Crie a estrutura de pastas necessÃ¡ria
mkdir -p ./dags ./logs ./plugins ./config ./data ./src ./notebooks

# Configure as permissÃµes (Linux/Mac)
echo -e "AIRFLOW_UID=$(id -u)" > .env
```

### 5ï¸âƒ£ Inicie os Containers Docker

```bash

# Inicie todos os serviÃ§os
docker-compose up -d
```

Aguarde alguns minutos para todos os serviÃ§os iniciarem.

### 6ï¸âƒ£ Verifique se tudo estÃ¡ rodando

```bash
docker-compose ps
```

VocÃª deve ver todos os serviÃ§os com status **`healthy`** ou **`running`**:
- airflow-apiserver
- airflow-scheduler
- airflow-worker
- airflow-triggerer
- redis

---

## ğŸ® Como Executar

### 1ï¸âƒ£ Acesse a Interface do Airflow

Abra seu navegador em: **http://localhost:8080**

**Credenciais padrÃ£o:**
- Username: `airflow`
- Password: `airflow`

### 2ï¸âƒ£ Ative a DAG

1. Na interface do Airflow, localize a DAG chamada **`youtube_weather_pipeline`**
2. Clique no botÃ£o de **Acionar/Trigger** para ativÃ¡-la
3. A DAG estÃ¡ configurada para executar **a cada 1 hora**

---

## ğŸ” Detalhamento das Etapas

### ğŸ“¥ **ETAPA 1: EXTRACT**

**Arquivo:** [`src/extract_data.py`](src/extract_data.py)

**O que faz:**
1. Faz uma requisiÃ§Ã£o HTTP GET para a API do OpenWeatherMap
2. Valida o status code da resposta (200 = sucesso)
3. Salva os dados brutos em formato JSON em `data/weather_data.json`

**Dados coletados:**
- Temperatura atual, mÃ­nima e mÃ¡xima
- SensaÃ§Ã£o tÃ©rmica
- Umidade e pressÃ£o atmosfÃ©rica
- Velocidade e direÃ§Ã£o do vento
- NÃ­vel de nuvens
- HorÃ¡rios de nascer e pÃ´r do sol
- Coordenadas geogrÃ¡ficas

---

### ğŸ”„ **ETAPA 2: TRANSFORM**

**Arquivo:** [`src/transform_data.py`](src/transform_data.py)

**O que faz:**

#### 2.1 **CriaÃ§Ã£o do DataFrame**
- LÃª o arquivo JSON
- Converte para DataFrame Pandas
- Normaliza dados aninhados usando `pd.json_normalize()`

#### 2.2 **NormalizaÃ§Ã£o da coluna 'weather'**
- A coluna `weather` vem como lista de dicionÃ¡rios
- Extrai: `weather_id`, `weather_main`, `weather_description`, `weather_icon`
- Concatena com o DataFrame principal

#### 2.3 **RemoÃ§Ã£o de colunas desnecessÃ¡rias**
```python
columns_to_drop = ['weather', 'weather_icon', 'sys.type']
```

#### 2.4 **RenomeaÃ§Ã£o de colunas**
PadronizaÃ§Ã£o para nomes claros em inglÃªs:
- `main.temp` â†’ `temperature`
- `main.humidity` â†’ `humidity`
- `coord.lon` â†’ `longitude`
- `sys.sunrise` â†’ `sunrise`
- E outros...

#### 2.5 **ConversÃ£o de timestamps**
Colunas convertidas de Unix timestamp para datetime:
```python
columns_to_normalize = ['datetime', 'sunrise', 'sunset']

# Converte para datetime do fuso horÃ¡rio de SÃ£o Paulo
df[col] = pd.to_datetime(df[col], unit='s', utc=True)
         .dt.tz_convert('America/Sao_Paulo')
```

**Resultado:** DataFrame limpo, estruturado e pronto para anÃ¡lise

---

### ğŸ’¾ **ETAPA 3: LOAD**

**Arquivo:** [`src/load_data.py`](src/load_data.py)

**O que faz:**

#### 3.1 **ConexÃ£o com o banco de dados**
```python
engine = create_engine(
    f"mysql+mysqlconnector://{user}:{quote_plus(password)}@{host}:3306/{database}"
)
```

#### 3.2 **InserÃ§Ã£o dos dados**
```python
df.to_sql(
    name='sp_weather',
    con=engine,
    if_exists='append',  # Adiciona novos registros
    index=False
)
```

#### 3.3 **ValidaÃ§Ã£o**
- Faz um `SELECT COUNT(*)` para verificar total de registros
- Loga o resultado para auditoria
---

## ğŸ“Š Fluxo da DAG no Airflow

**Arquivo:** [`dags/weather_dag.py`](dags/weather_dag.py)

### ConfiguraÃ§Ã£o da DAG

```python
@dag(
    dag_id='youtube_weather_pipeline',
    schedule='0 */1 * * *',  # Executa a cada 1 hora
    start_date=datetime(2026, 2, 7),
    catchup=False,  # NÃ£o executa datas passadas
    tags=['weather', 'etl']
)
```

### Tasks Definidas

```python
@task
def extract():
    extract_weather_data(url)

@task
def transform():
    df = data_transformations()
    df.to_parquet('/opt/airflow/data/temp_data.parquet')

@task
def load():
    df = pd.read_parquet('/opt/airflow/data/temp_data.parquet')
    load_weather_data('sp_weather', df)

# DependÃªncias
extract() >> transform() >> load()
```

**Por que usar Parquet entre transform e load?**
- Formato binÃ¡rio eficiente
- Preserva tipos de dados (datetime, float, etc.)
- Evita problemas com serializaÃ§Ã£o do Airflow

---

## ğŸ§ª Testes Locais (sem Airflow)

Para testar o pipeline sem Docker:

```bash
# Instale as dependÃªncias
uv pip install -e .

# Execute o script de teste
uv run main.py
```

> **Nota:** O arquivo `main.py` estÃ¡ comentado por padrÃ£o. Descomente-o para usar.

---

## ğŸ›‘ Parar e Limpar

### Parar os containers
```bash
docker-compose down
```

### Parar e remover volumes (ATENÃ‡ÃƒO: apaga os dados!)
```bash
docker-compose down -v
```

### Remover dados do Airflow mas manter o MySQL
```bash
docker-compose down
rm -rf logs/*
```
